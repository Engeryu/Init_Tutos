# ğŸ“Š End-to-End Data Science Workflow  
**Auteur :** Gaspard-Fauvelle Angel
**Licence :** [CC BY-NC-SA 2.0](https://creativecommons.org/licenses/by-nc-sa/2.0/)

Here is the english version of the document [End-to-End Data Science Workflow](/.End-to-End_Data_Science_Workflow_Eng.md)

Ce document est le dernier d'une sÃ©rie :
- [SQL_CheatSheet_FR.ipynb](SQL_CheatSheet_FR.md)
- [Python_Pandas_FR.ipynb](Python_Pandas_CheatSheet_FR.md)
- [Numpy_Pandas_FR](NumPy_Pandas_CheatSheet_FR.md)
- [Data_Preprocessing_Methods_FR.ipynb](Data_Preprocessing_Methods_FR.md)

# ğŸ§  Table des MatiÃ¨res Dynamique
1. [ğŸ§­ I. Introduction au Sujet](#ğŸ§­-I.-Introduction-au-Sujet)
   - [1.1. Contexte de l'Ã‰tude / Objectifs / Questions de Recherche](#1.1.-Contexte-de-lÃ‰tude-/-Objectifs-/-Questions-de-Recherche)
   - [1.2. Contraintes et Limitations](#1.2.-Contraintes-et-Limitations)
2. [ğŸ—‚ï¸ II. Configuration de l'Environnement](#ğŸ—‚ï¸-II.-Configuration-de-lEnvironnement)
   - [2.1. Imports](#2.1.-Imports)
   - [2.1.1. BibliothÃ¨ques](#2.1.1.-BibliothÃ¨ques)
   - [2.1.2. Chargement du Dataset](#2.1.2.-Chargement-du-Dataset)
   - [ğŸ‘ï¸ 2.2. Exploration Initiale - AperÃ§u de la Structure de Base du Dataset](#ğŸ‘ï¸-2.2.-Exploration-Initiale---AperÃ§u-de-la-Structure-de-Base-du-Dataset)
3. [ğŸ§¹ III. Nettoyage et Transformation des DonnÃ©es - Gestion des DonnÃ©es PolluÃ©es](#ğŸ§¹-III.-Nettoyage-et-Transformation-des-DonnÃ©es---Gestion-des-DonnÃ©es-PolluÃ©es)
4. [ğŸ§  IV. IngÃ©nierie des CaractÃ©ristiques](#ğŸ§ -IV.-IngÃ©nierie-des-CaractÃ©ristiques)
5. [ğŸ“ˆ V. Analyse Exploratoire des DonnÃ©es - EDA](#ğŸ“ˆ-V.-Analyse-Exploratoire-des-DonnÃ©es---EDA)
6. [ğŸ§ª VI. Analyse InfÃ©rentielle](#ğŸ§ª-VI.-Analyse-InfÃ©rentielle)
7. [ğŸ¤– VII. ModÃ©lisation PrÃ©dictive](#ğŸ¤–-VII.-ModÃ©lisation-PrÃ©dictive)
8. [ğŸ“Œ VIII. Analyse Prescriptive](#ğŸ“Œ-VIII.-Analyse-Prescriptive)

---

## ğŸ§­ I. Introduction au Sujet
### 1.1. Contexte de l'Ã‰tude / Objectifs / Questions de Recherche
Pourquoi avez-vous choisi ce dataset ? Quels sont les parties prenantes ? Quels sont les livrables Ã  la fin ?
### 1.2. Contraintes et Limitations
- Planification de votre Ã©chÃ©ance / Combien de temps pour travailler dessus.
- Ressources de calcul, qualitÃ© des donnÃ©es et granularitÃ©

---

## ğŸ—‚ï¸ II. Configuration de l'Environnement
### 2.1. Imports
#### 2.1.1. BibliothÃ¨ques
```python
# Importer les bibliothÃ¨ques nÃ©cessaires
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Importer les bibliothÃ¨ques secondaires
from sklearn.sublibraries import MachineLearningsTools as mltools...
from scipy.sublibrairies import MathsTools as mathtools...
from datetime import date, datetime...

# Importer les bibliothÃ¨ques de support
```

2.1.2. Chargement du Dataset
# Charger le dataset dans un DataFrame Pandas
df = pd.read_csv('votre_dataset.csv') # Pour lire un fichier CSV

# Lire les donnÃ©es depuis une URL
df_url = pd.read_csv('https://example.com/vos_donnees.csv') # Pour lire les donnÃ©es depuis une URL

# Lire un fichier Excel
df_excel = pd.read_excel('votre_fichier.xlsx', sheet_name='Feuille1') # Pour lire un fichier Excel

# Lire un fichier JSON
df_json = pd.read_json('votre_fichier.json') # Pour lire un fichier JSON

# Lire un fichier SQLite
import sqlite3
con = sqlite3.connect("database.sqlite") # Connecter Jupyter Ã  database.sqlite
df = pd.read_sql_query("SELECT * from table", con) # CrÃ©er un DataFrame Ã  partir d'une requÃªte SQL dans la derniÃ¨re variable

# Lire un fichier SQL (MÃªme exemple que ci-dessus, avec une autre API)
from sqlalchemy import create_engine # Ã€ insÃ©rer dans les bibliothÃ¨ques secondaires
engine = create_engine('sqlite:///votre_database.db')      # CrÃ©er un moteur de base de donnÃ©es SQLite
df_sql = pd.read_sql('SELECT * FROM votre_table', engine)  # Lire les donnÃ©es d'une table SQL

# VÃ©rifier que le rÃ©sultat de la requÃªte SQL est stockÃ© dans le dataframe
print(df.head())

# Lire un fichier Parquet
df_parquet = pd.read_parquet('votre_fichier.parquet') # Pour lire un fichier Parquet

# Les types de fichiers ci-dessus sont les principaux types de fichiers que vous rencontrerez en Data Science. Mais voici quelques autres types de fichiers ci-dessous comme :

# Lire un fichier HDF5
df_hdf5 = pd.read_hdf('votre_fichier.h5', key='votre_clÃ©') # Pour lire un fichier HDF5

# Lire un fichier CERN ROOT
#from rootpy.io import ROOTFile # Ã€ insÃ©rer dans les bibliothÃ¨ques secondaires
df_root = ROOT.TFile.Open('votre_fichier.root') # Pour lire un fichier ROOT

# Lire un fichier Feather
df_feather = pd.read_feather('votre_fichier.feather') # Pour lire un fichier Feather

# Lire un fichier Ã  largeur fixe
#column_widths = [10, 15, 20]    ## Pour dÃ©finir les largeurs des colonnes
df_fixed_width = pd.read_fwf('votre_fichier.txt', widths=column_widths) # Pour lire un fichier Ã  largeur fixe

# Lire des donnÃ©es depuis le presse-papiers
df_clipboard = pd.read_clipboard() # Pour lire des donnÃ©es depuis le presse-papiers

### ğŸ‘ï¸ 2.2. Exploration Initiale - AperÃ§u de la Structure de Base du Dataset
#### 2.2.1. Inspecter les dimensions et les types de donnÃ©es (Dictionnaire des variables avec .info(), et diagnostic approfondi de memory_usage)
```python
df.info(memory_usage='deep', show_counts='True')
```
#### 2.2.2. Inspecter un nombre fixe de lignes alÃ©atoires avec .sample()
```python
df.sample(5)
```

---

## ğŸ§¹ III. Nettoyage et Transformation des DonnÃ©es - Gestion des DonnÃ©es PolluÃ©es
### 3.1. Gestion des Valeurs Manquantes (.isna(), si les donnÃ©es manquantes sont >x% ou <y%)
La meilleure mÃ©thode est d'ajouter une dimension aux colonnes avec des valeurs manquantes en crÃ©ant une colonne boolÃ©enne (1 pour les valeurs manquantes et 0 pour les valeurs remplies)  
La deuxiÃ¨me mÃ©thode est, si les donnÃ©es manquantes sont <30%, d'utiliser la mÃ©diane ou le mode (prÃ©fÃ©rable) pour remplir les valeurs manquantes  
En dernier recours, si les donnÃ©es manquantes sont >30%, supprimer ces lignes avec .dropna(), pour les colonnes, si les donnÃ©es manquantes sont >40%, .dropna('ColonnesASupprimer', axis=1)  
"Ces seuils (par exemple 30% pour les lignes, 40% pour les colonnes) sont empiriques et doivent Ãªtre adaptÃ©s en fonction des connaissances du domaine, de la taille de l'Ã©chantillon et de la sensibilitÃ© du modÃ¨le."  
### 3.2. Gestion des Lignes en Double (.duplicated() et .drop_duplicates())
### 3.3. Gestion des Valeurs Aberrantes (MÃ©thode IQR & Z_scores)
```python
# DÃ©tection pour la loi de distribution
z_scores = ((data['colonne1'] - moyennes) / Ã©carts_types)
z_outliers = (np.abs(z_scores) > seuil_zscore).any(axis=1)

# DÃ©tection pour presque tout
q1, q3 = np.percentile(df['colonne1'], [25, 75])
iqr = q3 - q1
limite_infÃ©rieure = q1 - 1.5 * iqr
limite_supÃ©rieure = q3 + 1.5 * iqr
iqr_outliers = df[(df['colonne1'] >= limite_infÃ©rieure) & (df['colonne1'] <= limite_supÃ©rieure)]

# Avant de penser Ã  supprimer les valeurs aberrantes, vÃ©rifiez si ces valeurs aberrantes sont des erreurs ou un signal valide (Ã©vÃ©nements rares)
df_sans_z_outliers = data[~z_outliers].copy()
df_sans_iqr_outliers = df[(df['colonne1'] >= limite_infÃ©rieure) & (df['colonne1'] <= limite_supÃ©rieure)].copy()
```
### 3.4. Conversion de Type de DonnÃ©es - .astype(), pour rÃ©duire l'utilisation de la mÃ©moire et optimiser les distinctions de colonnes
```python
# Utiliser un dictionnaire pour stocker les colonnes comme clÃ©s et les types que vous souhaitez dÃ©finir comme valeurs
dict_convert = {
    'colonne1': 'dtypes'
}
# CrÃ©er une boucle pour appliquer la mÃ©thode .astype(valeurs_du_dict)
common_cols = set(df.columns) & set(dict_convert.keys())
# VÃ©rifier que les colonnes spÃ©cifiÃ©es dans le dictionnaire 'conversions' existent bien dans le DataFrame avant de les convertir
for col in common_cols:
    df[col] = df[col].astype(dict_convert[col])
```
### 3.5. Gestion des SÃ©ries Temporelles (dates avec DateTimeIndex())
```python
df.set_index(df['colonneDate'], inplace=True)
```

---

##  ğŸ§  IV. IngÃ©nierie des CaractÃ©ristiques
### 4.1. CrÃ©ation de Nouvelles CaractÃ©ristiques - NumÃ©riques, CatÃ©gorielles et/ou SÃ©ries Temporelles
```python
# ComprÃ©hension de liste pour trier les types de donnÃ©es / Les sÃ©ries temporelles (dates) peuvent Ãªtre utilisÃ©es comme index, converties en ordinal/catÃ©gorie
types_sÃ©ries_temporelles = ['datetime64']
caractÃ©ristiques_numÃ©riques = df.select_dtypes(include=[np.number]).columns.tolist()
caractÃ©ristiques_catÃ©gorielles = df.select_dtypes(exclude=np.number).columns.tolist()
caractÃ©ristiques_sÃ©ries_temporelles = df.select_dtypes(include=types_sÃ©ries_temporelles).columns.tolist()
```
### 4.2. Transformation des CaractÃ©ristiques
#### 4.2.1. Encodage des Variables CatÃ©gorielles
MÃ©thodes de PrÃ©traitement des DonnÃ©es

#### 4.2.2. Mise Ã  l'Ã‰chelle et Normalisation / Standardisation des Variables NumÃ©riques
MÃ©thodes de PrÃ©traitement des DonnÃ©es

#### 4.2.3. DiffÃ©renciation / Extraction de CaractÃ©ristiques des Variables de SÃ©ries Temporelles
MÃ©thodes de PrÃ©traitement des DonnÃ©es

---

## ğŸ“ˆ V. Analyse Exploratoire des DonnÃ©es - EDA
### 5.1. Analyse UnivariÃ©e (Statistiques Descriptives)
#### 5.1.1. CaractÃ©ristiques NumÃ©riques (Quantitatives)
```python
# Imprimer les descriptions statistiques
```
#### 5.1.2. CaractÃ©ristiques CatÃ©gorielles (Qualitatives)
#### 5.1.3. CaractÃ©ristiques de SÃ©ries Temporelles (Date)
```python
# FrÃ©quences, pourcentages, fractions et/ou frÃ©quences relatives
```
### 5.2. Analyse BivariÃ©e / MultivariÃ©e
```python
# RÃ©Ã©chantillonnage (.resample()), Moyenne Mobile (.rolling()), Fonction PondÃ©rÃ©e Exponentielle (.ewm())
```
#### 5.2.1. NumÃ©rique vs NumÃ©rique
```python
# Graphiques comparatifs entre 2 variables ou plus (Nuage de Points, Courbes de RÃ©gression/Tendances, Graphique Ã  Bulles, LmPlot, PairPlot, histogramme, etc...)
```
#### 5.2.2. CatÃ©gorielle vs CatÃ©gorielle
```python
# Graphiques comparatifs entre 2 variables ou plus (Histogramme GroupÃ©/EmpilÃ©, MosaicPlot, Heatmap, PairPlot, etc...)
```
#### 5.2.3. CatÃ©gorielle vs NumÃ©rique
```python
# Graphiques comparatifs entre 2 variables ou plus (BoxPlot, ViolinPlot, Barres de Moyenne/Erreur, Dot/Strip Plot, CatPlot, PairPlot, histogramme, etc...)
```
#### 5.2.4. SÃ©ries Temporelles vs NumÃ©rique/CatÃ©gorielle
```python
# Graphiques comparatifs entre 2 variables ou plus (statsmodels.tsa.seasonal_decompose, Histogramme, PairPlot, etc...)
```

---

## ğŸ§ª VI. Analyse InfÃ©rentielle
### 6.1. DÃ©finition des HypothÃ¨ses
DÃ©finissez votre HypothÃ¨se Nulle (H0)  
### 6.2. Tests d'HypothÃ¨ses
#### 6.2.1. Tests d'HypothÃ¨se de NormalitÃ© et d'HomoscÃ©dasticitÃ© (homogÃ©nÃ©itÃ© de la variance) (Tests prÃ©liminaires avant de choisir des Tests ParamÃ©triques ou Non ParamÃ©triques)
```python
# Tests de NormalitÃ© : Test de Shapiro-Wilk, Test de Kolmogorov-Smirnov, Test d'Anderson-Darling

# Tests d'HomoscÃ©dasticitÃ© : Test de Levene, Test de Bartlett

# Test d'AutocorrÃ©lation : Test de Durbin-Watson, Test de White

# Test de MulticolinÃ©aritÃ© : VIF (Facteur d'Inflation de la Variance)
```
#### 6.2.2. Tests ParamÃ©triques (DonnÃ©es sans valeurs aberrantes, suivant une loi de distribution)
```python
# Pour les donnÃ©es uniquement numÃ©riques (Analyse de CorrÃ©lation : CorrÃ©lation de Pearson, Test T de RÃ©gression LinÃ©aire)

# Pour les donnÃ©es uniquement catÃ©gorielles (Intervalles de Confiance) : RÃ©gression Logistique, ModÃ¨les Log-LinÃ©aires

# Pour les donnÃ©es numÃ©riques et catÃ©gorielles (Analyse de la Variance) : Test T de Student, MANOVA/ANOVA (Analyse de la Variance)
```
#### 6.2.3. Tests Non ParamÃ©triques (Robustes face aux donnÃ©es avec valeurs aberrantes et indiffÃ©rents Ã  la loi de distribution)
```python
# Pour les donnÃ©es uniquement numÃ©riques (Analyse de CorrÃ©lation) : CorrÃ©lation de Spearman, CorrÃ©lation de Kendall (Tau)

# Pour les donnÃ©es uniquement catÃ©gorielles (Analyse de la Variance) : ChiÂ², Test Exact de Fisher, Test de McNemar, Test de Cochran-Armitage pour les Tendances (Analyse de CorrÃ©lation)

# Pour les donnÃ©es numÃ©riques et catÃ©gorielles (Analyse de la Variance) : Test de Mann-Whitney, Test des Rangs SignÃ©s de Wilcoxon, ANOVA de Kruskal-Wallis
```
#### 6.3. Conclusion des HypothÃ¨ses
#### 6.3.1. H0 ou H1 est valide
#### 6.3.2. SÃ©lection des CaractÃ©ristiques
```python
# Ã€ partir des diffÃ©rents tests, quelles colonnes allez-vous conserver ?
df = df[['colonne1', 'colonne2', '...', 'colonneN']]
# Si vous avez plus de colonnes Ã  conserver qu'Ã  supprimer, quelles colonnes allez-vous supprimer ?
df = df.drop(['colonne1', 'colonne2', '...', 'colonneN'], axis=1])
```

---

## ğŸ¤– VII. ModÃ©lisation PrÃ©dictive
### 7.1. PrÃ©traitement des DonnÃ©es pour la ModÃ©lisation
#### 7.1.1. SÃ©paration des CaractÃ©ristiques / Cible
```python
# CaractÃ©ristiques & Cible
X = df.drop(['ColonneCible'], axis=1)
y = df[['ColonneCible']]
```
#### 7.1.2. SÃ©paration EntraÃ®nement-Test (train_test_split comme tts) et SÃ©paration des SÃ©ries Temporelles (TimeSeriesSplit comme tss)
```python
# La partition commune est 80/20 (80% pour l'EntraÃ®nement et 20% pour le Test), le mÃ©lange doit Ãªtre dÃ©fini comme True
X_train, X_test, y_train, y_test = tts(X, y, train_size=0.8, random_state=1, shuffle=True, stratify=None)
```
#### 7.1.3. StratÃ©gie de Validation CroisÃ©e (RÃ©glage des HyperparamÃ¨tres)
```python
# HyperparamÃ¨tres globaux : Recherche en Grille (GridSearchCV), Recherche AlÃ©atoire (RandomizedSearchCV), Optimisation BayÃ©sienne (BayesianSearchCV Optuna, Hyperopt)

# HyperparamÃ¨tre de rÃ©gression uniquement : RÃ©glage intÃ©grÃ© de la validation croisÃ©e

# HyperparamÃ¨tre de classificateur uniquement : Validation croisÃ©e avec Ã©chantillonnage stratifiÃ© (StratifiedKFold)
```
### 7.2. EntraÃ®nement et Ã‰valuation du ModÃ¨le
```python
# RÃ©gresseurs (par exemple, RÃ©gression LinÃ©aire, RÃ©gression des ForÃªts AlÃ©atoires, RÃ©gression par Gradient Boosting...)

# Classificateurs (par exemple, RÃ©gression Logistique, Arbres de DÃ©cision, Classificateur de ForÃªts AlÃ©atoires, Machines Ã  Vecteurs de Support)

# Pipelines pour le prÃ©traitement et l'entraÃ®nement

# Techniques de validation croisÃ©e (par exemple, Group/K Fold, StratifiedKFold, TimeSeriesSplit pour les donnÃ©es sÃ©quentielles)
```
#### 7.2.1. EntraÃ®nement du ModÃ¨le sur l'Ensemble d'EntraÃ®nement
```python
# RÃ©gresseurs (par exemple, RÃ©gression LinÃ©aire, RÃ©gression des ForÃªts AlÃ©atoires, RÃ©gression par Gradient Boosting...)

# Classificateurs (par exemple, RÃ©gression Logistique, Arbres de DÃ©cision, Classificateur de ForÃªts AlÃ©atoires, Machines Ã  Vecteurs de Support)

# Pipelines pour le prÃ©traitement et l'entraÃ®nement

# Techniques de validation croisÃ©e (par exemple, Group/K Fold, StratifiedKFold, TimeSeriesSplit pour les donnÃ©es sÃ©quentielles)
```
#### 7.2.2. Analyse des Erreurs et MÃ©triques
```python
# MÃ©triques de RÃ©gression : Erreur Absolue Moyenne (MAE), Erreur Quadratique Moyenne (MSE), Erreur Quadratique Moyenne Racine (RMSE), R-carrÃ© (RÂ²)

# Analyse du Classificateur : PrÃ©cision, Rappel, Score F1 et Analyse Visuelle des Erreurs comme la Matrice de Confusion, Courbe ROC / AUC, Courbes PrÃ©cision-Rappel
```
#### 7.2.3. Ã‰valuation Visuelle
```python
# RÃ©gression : Nuages de Points (rÃ©el vs prÃ©dit), Graphiques en Ligne, Graphiques des RÃ©sidus, Courbes d'Apprentissage, Box Plots

# Classificateur : Heatmaps (pour les Matrices de Confusion), Courbes ROC, Courbes PrÃ©cision-Rappel
```
### 7.3. GÃ©nÃ©ralisation et Cas d'Utilisation RÃ©els
```python
# RÃ©gression : Nuages de Points (rÃ©el vs prÃ©dit), Graphiques en Ligne, Graphiques des RÃ©sidus, Courbes d'Apprentissage, Box Plots

# Classificateur : Heatmaps (pour les Matrices de Confusion), Courbes ROC, Courbes PrÃ©cision-Rappel

```
#### 7.3.1. PrÃ©diction sur les Ensembles de Test RÃ©els
```python
# InfÃ©rence sur des donnÃ©es non vues/du monde rÃ©el en utilisant le modÃ¨le entraÃ®nÃ©

# Utiliser les pipelines de prÃ©traitement/ingÃ©nierie des caractÃ©ristiques sur les donnÃ©es de test rÃ©elles

# GÃ©nÃ©rer des prÃ©dictions via model.predict (par lots ou en continu)

# Effectuer l'Ã©valuation finale et l'analyse des erreurs sur les performances de l'ensemble de test rÃ©el
```
#### 7.3.2. ConsidÃ©rations de DÃ©ploiement (facultatif)
```python
# SÃ©rialisation et exportation du modÃ¨le (par exemple, pickle, joblib, ONNX, SavedModel)

# Cadres de dÃ©ploiement d'API (par exemple, Flask, FastAPI, TensorFlow Serving, TorchServe)

# Conteneurisation et orchestration (par exemple, Docker, Kubernetes)

# Surveillance, journalisation et optimisation des performances pour la production
```
## ğŸ“Œ VIII. Analyse Prescriptive
### 8.1. Recommandations Commerciales
- Traduire les insights analytiques en stratÃ©gies exploitables.
- Utiliser le storytelling de donnÃ©es pour informer la prise de dÃ©cision stratÃ©gique.
- Utiliser des cadres tels que l'analyse SWOT, la dÃ©finition des KPI et la surveillance des performances.
- Engager des experts du domaine pour valider les recommandations.
### 8.2. ScÃ©narios HypothÃ©tiques / Simulations
- DÃ©velopper des modÃ¨les de simulation pour explorer des scÃ©narios futurs alternatifs.
- Employer des mÃ©thodes comme les simulations de Monte Carlo et l'analyse de sensibilitÃ©.
- Utiliser la planification de scÃ©narios pour Ã©valuer l'impact de diffÃ©rentes hypothÃ¨ses.
- Ã‰valuer les risques et les incertitudes dans diverses situations opÃ©rationnelles ou de marchÃ©.
### 8.3. Conception de Tableaux de Bord (Facultatif pour les Parties Prenantes)
- Construire des tableaux de bord interactifs et informatifs pour la visualisation des donnÃ©es en temps rÃ©el.
- Utiliser des outils tels que Tableau, PowerBI ou Plotly Dash pour la facilitÃ© d'utilisation et l'accessibilitÃ©.
- Se concentrer sur les principes de conception centrÃ©e sur l'utilisateur : clartÃ©, simplicitÃ© et insights exploitables.
- Assurer l'Ã©volutivitÃ©, la maintenabilitÃ© et la sÃ©curitÃ© des donnÃ©es dans le dÃ©ploiement des tableaux de bord.

---

ğŸ‰ FÃ©licitations pour avoir suivi ce guide de Data Science de bout en bout !  
Vous Ãªtes maintenant prÃªt(e) Ã  passer Ã  l'Ã©tape suivante et publier votre projet.  
Rendez-vous sur [Git & GitHub Environment](01-Github_Git_Init.md) pour apprendre Ã  importer votre travail sur GitHub et le rendre visible.  


_Edited: 2025-04-22_